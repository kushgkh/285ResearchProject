{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "# env = gym.make(ENV_NAME)\n",
    "env = sim.Market(0.9, sim.sinFunc)\n",
    "# np.random.seed(123)\n",
    "# env.seed(123)\n",
    "# nb_actions = env.action_space.n\n",
    "nb_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ramimostafa/anaconda3/envs/cs285_env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 227\n",
      "Trainable params: 227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + (10,)))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reset() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2a48c2cb29a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cs285_env/lib/python3.7/site-packages/keras_rl-0.4.2-py3.7.egg/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                         observation = self.processor.process_observation(\n",
      "\u001b[0;31mTypeError\u001b[0m: reset() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 85.000, steps: 85\n",
      "Episode 2: reward: 153.000, steps: 153\n",
      "Episode 3: reward: 57.000, steps: 57\n",
      "Episode 4: reward: 72.000, steps: 72\n",
      "Episode 5: reward: 143.000, steps: 143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13061c9d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "#         self.exploration_rate = 1.0 # Initial exploration rate\n",
    "#         self.exploration_delta = 1.0 / iterations # Shift from exploration to explotation\n",
    "\n",
    "        # Input has five neurons, each represents single game state (0-4)\n",
    "        self.input_count = 10\n",
    "        # Output is two neurons, each represents Q-value for action (1 and 0)\n",
    "        self.output_count = 1\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "\n",
    "    # Define tensorflow model graph\n",
    "    def define_model(self):\n",
    "        # Input is an array of 5 items (state one-hot)\n",
    "        # Input is 2-dimensional, due to possibility of batched training data\n",
    "        # NOTE: In this example we assume no batching.\n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[None, self.input_count])\n",
    "\n",
    "        # Two hidden layers of 16 neurons with sigmoid activation initialized to zero for stability\n",
    "        fc1 = tf.layers.dense(self.model_input, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((self.input_count, 16))))\n",
    "        fc2 = tf.layers.dense(fc1, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((16, self.output_count))))\n",
    "\n",
    "        # Output is two values, Q for both possible actions FORWARD and BACKWARD\n",
    "        # Output is 2-dimensional, due to possibility of batched training data\n",
    "        # NOTE: In this example we assume no batching.\n",
    "        self.model_output = tf.layers.dense(fc2, self.output_count)\n",
    "\n",
    "        # This is for feeding training output (a.k.a ideal target values)\n",
    "        self.target_output = tf.placeholder(shape=[None, self.output_count], dtype=tf.float32)\n",
    "        # Loss is mean squared difference between current output and ideal target values\n",
    "        loss = tf.losses.mean_squared_error(self.target_output, self.model_output)\n",
    "        # Optimizer adjusts weights to minimize loss, with the speed of learning_rate\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        # Initializer to set weights to initial values\n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "\n",
    "    # Ask model to estimate Q value for specific state (inference)\n",
    "    def get_Q(self, state):\n",
    "        # Model input: Single state represented by array of 5 items (state one-hot)\n",
    "        # Model output: Array of Q values for single state\n",
    "        return self.session.run(self.model_output, feed_dict={self.model_input: self.to_one_hot(state)})[0]\n",
    "\n",
    "    # Turn state into 2d one-hot tensor\n",
    "    # Example: 3 -> [[0,0,0,1,0]]\n",
    "    def to_one_hot(self, state):\n",
    "        one_hot = np.zeros((1, 5))\n",
    "        one_hot[0, [state]] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action()\n",
    "\n",
    "    # Which action (FORWARD or BACKWARD) has bigger Q-value, estimated by our model (inference).\n",
    "    def greedy_action(self, state):\n",
    "        # argmax picks the higher Q-value and returns the index (FORWARD=0, BACKWARD=1)\n",
    "        return np.argmax(self.get_Q(state))\n",
    "\n",
    "    def random_action(self):\n",
    "        return 0 if random.random() < 0.5 else 1\n",
    "\n",
    "    def train(self, old_state, action, reward, new_state):\n",
    "        # Ask the model for the Q values of the old state (inference)\n",
    "        old_state_Q_values = self.get_Q(old_state)\n",
    "\n",
    "        # Ask the model for the Q values of the new state (inference)\n",
    "        new_state_Q_values = self.get_Q(new_state)\n",
    "\n",
    "        # Real Q value for the action we took. This is what we will train towards.\n",
    "        old_state_Q_values[action] = reward + self.discount * np.amax(new_state_Q_values)\n",
    "        \n",
    "        # Setup training data\n",
    "        training_input = self.to_one_hot(old_state)\n",
    "        target_output = [old_state_Q_values]\n",
    "        training_data = {self.model_input: training_input, self.target_output: target_output}\n",
    "\n",
    "        # Train\n",
    "        self.session.run(self.optimizer, feed_dict=training_data)\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Train our model with new data\n",
    "        self.train(old_state, action, reward, new_state)\n",
    "\n",
    "        # Finally shift our exploration_rate toward zero (less gambling)\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta\n",
    "view rawdeep_gambler.py hosted with ❤ by GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
