{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# from modelAny import *\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/ramimostafa/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ramimostafa/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done], 1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 26.666667. action: 1.000000. mean reward 26.666667.\n",
      "World Perf: Episode 7.000000. Reward 30.666667. action: 1.000000. mean reward 26.706667.\n",
      "World Perf: Episode 10.000000. Reward 27.000000. action: 1.000000. mean reward 26.709600.\n",
      "World Perf: Episode 13.000000. Reward 26.000000. action: 0.000000. mean reward 26.702504.\n",
      "World Perf: Episode 16.000000. Reward 35.333333. action: 1.000000. mean reward 26.788812.\n",
      "World Perf: Episode 19.000000. Reward 30.000000. action: 0.000000. mean reward 26.820924.\n",
      "World Perf: Episode 22.000000. Reward 28.666667. action: 1.000000. mean reward 26.839382.\n",
      "World Perf: Episode 25.000000. Reward 21.333333. action: 1.000000. mean reward 26.784321.\n",
      "World Perf: Episode 28.000000. Reward 20.666667. action: 0.000000. mean reward 26.723145.\n",
      "World Perf: Episode 31.000000. Reward 13.000000. action: 0.000000. mean reward 26.585913.\n",
      "World Perf: Episode 34.000000. Reward 15.666667. action: 0.000000. mean reward 26.476721.\n",
      "World Perf: Episode 37.000000. Reward 30.000000. action: 1.000000. mean reward 26.511953.\n",
      "World Perf: Episode 40.000000. Reward 19.000000. action: 0.000000. mean reward 26.436834.\n",
      "World Perf: Episode 43.000000. Reward 18.666667. action: 0.000000. mean reward 26.359132.\n",
      "World Perf: Episode 46.000000. Reward 19.333333. action: 1.000000. mean reward 26.288874.\n",
      "World Perf: Episode 49.000000. Reward 21.666667. action: 0.000000. mean reward 26.242652.\n",
      "World Perf: Episode 52.000000. Reward 18.666667. action: 1.000000. mean reward 26.166892.\n",
      "World Perf: Episode 55.000000. Reward 18.333333. action: 0.000000. mean reward 26.088557.\n",
      "World Perf: Episode 58.000000. Reward 17.666667. action: 0.000000. mean reward 26.004338.\n",
      "World Perf: Episode 61.000000. Reward 16.333333. action: 1.000000. mean reward 25.907628.\n",
      "World Perf: Episode 64.000000. Reward 30.000000. action: 0.000000. mean reward 25.948552.\n",
      "World Perf: Episode 67.000000. Reward 13.000000. action: 1.000000. mean reward 25.819066.\n",
      "World Perf: Episode 70.000000. Reward 37.666667. action: 1.000000. mean reward 25.937542.\n",
      "World Perf: Episode 73.000000. Reward 24.000000. action: 1.000000. mean reward 25.918167.\n",
      "World Perf: Episode 76.000000. Reward 23.333333. action: 0.000000. mean reward 25.892318.\n",
      "World Perf: Episode 79.000000. Reward 16.000000. action: 0.000000. mean reward 25.793395.\n",
      "World Perf: Episode 82.000000. Reward 18.666667. action: 1.000000. mean reward 25.722128.\n",
      "World Perf: Episode 85.000000. Reward 21.666667. action: 1.000000. mean reward 25.681573.\n",
      "World Perf: Episode 88.000000. Reward 24.333333. action: 0.000000. mean reward 25.668091.\n",
      "World Perf: Episode 91.000000. Reward 25.666667. action: 1.000000. mean reward 25.668077.\n",
      "World Perf: Episode 94.000000. Reward 30.666667. action: 1.000000. mean reward 25.718062.\n",
      "World Perf: Episode 97.000000. Reward 27.666667. action: 0.000000. mean reward 25.737548.\n",
      "World Perf: Episode 100.000000. Reward 22.333333. action: 1.000000. mean reward 25.703506.\n",
      "World Perf: Episode 103.000000. Reward 28.333333. action: 0.000000. mean reward 25.729805.\n",
      "World Perf: Episode 106.000000. Reward 19.666667. action: 1.000000. mean reward 25.554146.\n",
      "World Perf: Episode 109.000000. Reward 41.000000. action: 0.000000. mean reward 25.567900.\n",
      "World Perf: Episode 112.000000. Reward 16.333333. action: 1.000000. mean reward 25.377859.\n",
      "World Perf: Episode 115.000000. Reward 24.666667. action: 1.000000. mean reward 25.288948.\n",
      "World Perf: Episode 118.000000. Reward 24.666667. action: 0.000000. mean reward 25.100149.\n",
      "World Perf: Episode 121.000000. Reward 27.000000. action: 1.000000. mean reward 26.028984.\n",
      "World Perf: Episode 124.000000. Reward 39.666667. action: 1.000000. mean reward 28.838808.\n",
      "World Perf: Episode 127.000000. Reward 42.666667. action: 1.000000. mean reward 28.787596.\n",
      "World Perf: Episode 130.000000. Reward 43.000000. action: 1.000000. mean reward 28.781120.\n",
      "World Perf: Episode 133.000000. Reward 30.333333. action: 0.000000. mean reward 28.738886.\n",
      "World Perf: Episode 136.000000. Reward 21.333333. action: 0.000000. mean reward 28.847176.\n",
      "World Perf: Episode 139.000000. Reward 16.000000. action: 1.000000. mean reward 28.507845.\n",
      "World Perf: Episode 142.000000. Reward 23.666667. action: 0.000000. mean reward 28.325661.\n",
      "World Perf: Episode 145.000000. Reward 17.333333. action: 1.000000. mean reward 27.999687.\n",
      "World Perf: Episode 148.000000. Reward 15.333333. action: 1.000000. mean reward 28.019697.\n",
      "World Perf: Episode 151.000000. Reward 23.333333. action: 0.000000. mean reward 27.742002.\n",
      "World Perf: Episode 154.000000. Reward 22.666667. action: 0.000000. mean reward 27.535002.\n",
      "World Perf: Episode 157.000000. Reward 24.666667. action: 1.000000. mean reward 27.324621.\n",
      "World Perf: Episode 160.000000. Reward 19.333333. action: 0.000000. mean reward 27.127031.\n",
      "World Perf: Episode 163.000000. Reward 26.333333. action: 1.000000. mean reward 27.047295.\n",
      "World Perf: Episode 166.000000. Reward 35.000000. action: 0.000000. mean reward 26.992561.\n",
      "World Perf: Episode 169.000000. Reward 13.333333. action: 1.000000. mean reward 27.737343.\n",
      "World Perf: Episode 172.000000. Reward 29.666667. action: 1.000000. mean reward 29.512781.\n",
      "World Perf: Episode 175.000000. Reward 31.333333. action: 1.000000. mean reward 29.303513.\n",
      "World Perf: Episode 178.000000. Reward 33.000000. action: 0.000000. mean reward 29.102676.\n",
      "World Perf: Episode 181.000000. Reward 30.666667. action: 0.000000. mean reward 28.999992.\n",
      "World Perf: Episode 184.000000. Reward 33.666667. action: 0.000000. mean reward 28.823565.\n",
      "World Perf: Episode 187.000000. Reward 25.333333. action: 0.000000. mean reward 28.563919.\n",
      "World Perf: Episode 190.000000. Reward 41.666667. action: 0.000000. mean reward 28.555399.\n",
      "World Perf: Episode 193.000000. Reward 19.000000. action: 0.000000. mean reward 28.334517.\n",
      "World Perf: Episode 196.000000. Reward 20.000000. action: 1.000000. mean reward 28.070108.\n",
      "World Perf: Episode 199.000000. Reward 33.000000. action: 1.000000. mean reward 28.047142.\n",
      "World Perf: Episode 202.000000. Reward 58.333333. action: 0.000000. mean reward 28.158823.\n",
      "World Perf: Episode 205.000000. Reward 15.000000. action: 0.000000. mean reward 27.844620.\n",
      "World Perf: Episode 208.000000. Reward 20.666667. action: 1.000000. mean reward 27.833021.\n",
      "World Perf: Episode 211.000000. Reward 37.666667. action: 0.000000. mean reward 27.717085.\n",
      "World Perf: Episode 214.000000. Reward 21.000000. action: 0.000000. mean reward 29.868622.\n",
      "World Perf: Episode 217.000000. Reward 30.666667. action: 1.000000. mean reward 31.881937.\n",
      "World Perf: Episode 220.000000. Reward 22.000000. action: 1.000000. mean reward 31.548294.\n",
      "World Perf: Episode 223.000000. Reward 37.000000. action: 1.000000. mean reward 31.376320.\n",
      "World Perf: Episode 226.000000. Reward 29.666667. action: 0.000000. mean reward 31.120430.\n",
      "World Perf: Episode 229.000000. Reward 25.333333. action: 0.000000. mean reward 30.786772.\n",
      "World Perf: Episode 232.000000. Reward 30.333333. action: 0.000000. mean reward 30.518532.\n",
      "World Perf: Episode 235.000000. Reward 27.000000. action: 1.000000. mean reward 30.276934.\n",
      "World Perf: Episode 238.000000. Reward 54.000000. action: 1.000000. mean reward 32.730534.\n",
      "World Perf: Episode 241.000000. Reward 54.666667. action: 0.000000. mean reward 35.787720.\n",
      "World Perf: Episode 244.000000. Reward 52.333333. action: 0.000000. mean reward 36.159584.\n",
      "World Perf: Episode 247.000000. Reward 34.666667. action: 0.000000. mean reward 40.433262.\n",
      "World Perf: Episode 250.000000. Reward 36.666667. action: 0.000000. mean reward 43.198849.\n",
      "World Perf: Episode 253.000000. Reward 24.333333. action: 1.000000. mean reward 42.698986.\n",
      "World Perf: Episode 256.000000. Reward 30.000000. action: 0.000000. mean reward 42.215424.\n",
      "World Perf: Episode 259.000000. Reward 51.333333. action: 0.000000. mean reward 52.638367.\n",
      "World Perf: Episode 262.000000. Reward 45.666667. action: 1.000000. mean reward 52.091839.\n",
      "World Perf: Episode 265.000000. Reward 52.000000. action: 0.000000. mean reward 51.669037.\n",
      "World Perf: Episode 268.000000. Reward 33.333333. action: 1.000000. mean reward 54.211025.\n",
      "World Perf: Episode 271.000000. Reward 26.666667. action: 1.000000. mean reward 53.422230.\n",
      "World Perf: Episode 274.000000. Reward 18.666667. action: 0.000000. mean reward 52.745270.\n",
      "World Perf: Episode 277.000000. Reward 40.666667. action: 0.000000. mean reward 52.231979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 280.000000. Reward 29.333333. action: 0.000000. mean reward 53.057018.\n",
      "World Perf: Episode 283.000000. Reward 34.000000. action: 0.000000. mean reward 54.176697.\n",
      "World Perf: Episode 286.000000. Reward 27.333333. action: 0.000000. mean reward 56.108547.\n",
      "World Perf: Episode 289.000000. Reward 54.333333. action: 0.000000. mean reward 55.888367.\n",
      "World Perf: Episode 292.000000. Reward 12.000000. action: 1.000000. mean reward 54.978394.\n",
      "World Perf: Episode 295.000000. Reward 50.333333. action: 1.000000. mean reward 54.641659.\n",
      "World Perf: Episode 298.000000. Reward 13.333333. action: 0.000000. mean reward 54.207981.\n",
      "World Perf: Episode 301.000000. Reward 23.000000. action: 0.000000. mean reward 55.727848.\n",
      "World Perf: Episode 304.000000. Reward 29.000000. action: 0.000000. mean reward 55.101505.\n",
      "World Perf: Episode 307.000000. Reward 66.000000. action: 1.000000. mean reward 54.919590.\n",
      "World Perf: Episode 310.000000. Reward 36.666667. action: 0.000000. mean reward 62.533070.\n",
      "World Perf: Episode 313.000000. Reward 33.666667. action: 0.000000. mean reward 64.103088.\n",
      "World Perf: Episode 316.000000. Reward 71.666667. action: 1.000000. mean reward 65.902443.\n",
      "World Perf: Episode 319.000000. Reward 48.666667. action: 1.000000. mean reward 68.211411.\n",
      "World Perf: Episode 322.000000. Reward 32.666667. action: 0.000000. mean reward 67.782349.\n",
      "World Perf: Episode 325.000000. Reward 33.333333. action: 0.000000. mean reward 67.659370.\n",
      "World Perf: Episode 328.000000. Reward 20.666667. action: 1.000000. mean reward 66.823669.\n",
      "World Perf: Episode 331.000000. Reward 58.000000. action: 1.000000. mean reward 66.187637.\n",
      "World Perf: Episode 334.000000. Reward 38.666667. action: 1.000000. mean reward 65.441414.\n",
      "World Perf: Episode 337.000000. Reward 26.333333. action: 0.000000. mean reward 64.500565.\n",
      "World Perf: Episode 340.000000. Reward 61.000000. action: 1.000000. mean reward 64.136497.\n",
      "World Perf: Episode 343.000000. Reward 46.333333. action: 1.000000. mean reward 65.487885.\n",
      "World Perf: Episode 346.000000. Reward 51.666667. action: 1.000000. mean reward 67.861931.\n",
      "World Perf: Episode 349.000000. Reward 35.333333. action: 1.000000. mean reward 69.731087.\n",
      "World Perf: Episode 352.000000. Reward 43.666667. action: 0.000000. mean reward 71.532150.\n",
      "World Perf: Episode 355.000000. Reward 39.000000. action: 0.000000. mean reward 71.158463.\n",
      "World Perf: Episode 358.000000. Reward 34.666667. action: 1.000000. mean reward 70.483063.\n",
      "World Perf: Episode 361.000000. Reward 39.000000. action: 1.000000. mean reward 69.963432.\n",
      "World Perf: Episode 364.000000. Reward 50.666667. action: 0.000000. mean reward 69.200272.\n",
      "World Perf: Episode 367.000000. Reward 63.333333. action: 1.000000. mean reward 68.854958.\n",
      "World Perf: Episode 370.000000. Reward 49.333333. action: 1.000000. mean reward 69.215630.\n",
      "World Perf: Episode 373.000000. Reward 21.000000. action: 1.000000. mean reward 68.316322.\n",
      "World Perf: Episode 376.000000. Reward 67.666667. action: 0.000000. mean reward 70.617653.\n",
      "World Perf: Episode 379.000000. Reward 58.333333. action: 0.000000. mean reward 70.232925.\n",
      "World Perf: Episode 382.000000. Reward 32.000000. action: 1.000000. mean reward 69.568062.\n",
      "World Perf: Episode 385.000000. Reward 23.666667. action: 1.000000. mean reward 68.580009.\n",
      "World Perf: Episode 388.000000. Reward 32.666667. action: 1.000000. mean reward 67.985802.\n",
      "World Perf: Episode 391.000000. Reward 78.666667. action: 1.000000. mean reward 67.801941.\n",
      "World Perf: Episode 394.000000. Reward 81.666667. action: 0.000000. mean reward 67.710381.\n",
      "World Perf: Episode 397.000000. Reward 22.000000. action: 0.000000. mean reward 69.609566.\n",
      "World Perf: Episode 400.000000. Reward 44.333333. action: 0.000000. mean reward 71.568245.\n",
      "World Perf: Episode 403.000000. Reward 29.666667. action: 0.000000. mean reward 73.323013.\n",
      "World Perf: Episode 406.000000. Reward 45.333333. action: 0.000000. mean reward 72.869156.\n",
      "World Perf: Episode 409.000000. Reward 29.333333. action: 0.000000. mean reward 72.434410.\n",
      "World Perf: Episode 412.000000. Reward 61.666667. action: 0.000000. mean reward 71.759361.\n",
      "World Perf: Episode 415.000000. Reward 33.333333. action: 1.000000. mean reward 70.756889.\n",
      "World Perf: Episode 418.000000. Reward 69.000000. action: 0.000000. mean reward 70.213165.\n",
      "World Perf: Episode 421.000000. Reward 68.666667. action: 0.000000. mean reward 69.657661.\n",
      "World Perf: Episode 424.000000. Reward 43.666667. action: 1.000000. mean reward 68.824539.\n",
      "World Perf: Episode 427.000000. Reward 60.000000. action: 1.000000. mean reward 68.216133.\n",
      "World Perf: Episode 430.000000. Reward 78.333333. action: 1.000000. mean reward 70.710793.\n",
      "World Perf: Episode 433.000000. Reward 78.666667. action: 1.000000. mean reward 72.942688.\n",
      "World Perf: Episode 436.000000. Reward 92.333333. action: 1.000000. mean reward 73.742355.\n",
      "World Perf: Episode 439.000000. Reward 88.333333. action: 0.000000. mean reward 73.394936.\n",
      "World Perf: Episode 442.000000. Reward 42.333333. action: 0.000000. mean reward 72.453590.\n",
      "World Perf: Episode 445.000000. Reward 95.000000. action: 1.000000. mean reward 74.759583.\n",
      "World Perf: Episode 448.000000. Reward 30.666667. action: 0.000000. mean reward 73.733528.\n",
      "World Perf: Episode 451.000000. Reward 75.000000. action: 1.000000. mean reward 73.662460.\n",
      "World Perf: Episode 454.000000. Reward 58.333333. action: 1.000000. mean reward 75.553978.\n",
      "World Perf: Episode 457.000000. Reward 65.000000. action: 0.000000. mean reward 77.756401.\n",
      "World Perf: Episode 460.000000. Reward 97.000000. action: 1.000000. mean reward 77.405052.\n",
      "World Perf: Episode 463.000000. Reward 68.333333. action: 1.000000. mean reward 79.266029.\n",
      "World Perf: Episode 466.000000. Reward 76.000000. action: 0.000000. mean reward 78.558525.\n",
      "World Perf: Episode 469.000000. Reward 53.333333. action: 1.000000. mean reward 77.732597.\n",
      "World Perf: Episode 472.000000. Reward 69.666667. action: 0.000000. mean reward 77.668388.\n",
      "World Perf: Episode 475.000000. Reward 55.000000. action: 1.000000. mean reward 77.488014.\n",
      "World Perf: Episode 478.000000. Reward 82.666667. action: 1.000000. mean reward 77.097496.\n",
      "World Perf: Episode 481.000000. Reward 59.333333. action: 1.000000. mean reward 77.145752.\n",
      "World Perf: Episode 484.000000. Reward 39.333333. action: 0.000000. mean reward 76.288979.\n",
      "World Perf: Episode 487.000000. Reward 83.666667. action: 1.000000. mean reward 75.729530.\n",
      "World Perf: Episode 490.000000. Reward 58.666667. action: 1.000000. mean reward 77.922234.\n",
      "World Perf: Episode 493.000000. Reward 112.333333. action: 0.000000. mean reward 77.709030.\n",
      "World Perf: Episode 496.000000. Reward 61.000000. action: 0.000000. mean reward 77.024979.\n",
      "World Perf: Episode 499.000000. Reward 28.000000. action: 1.000000. mean reward 75.902504.\n",
      "World Perf: Episode 502.000000. Reward 74.000000. action: 1.000000. mean reward 75.285400.\n",
      "World Perf: Episode 505.000000. Reward 79.666667. action: 0.000000. mean reward 74.677971.\n",
      "World Perf: Episode 508.000000. Reward 84.000000. action: 0.000000. mean reward 74.166313.\n",
      "World Perf: Episode 511.000000. Reward 99.333333. action: 1.000000. mean reward 77.613205.\n",
      "World Perf: Episode 514.000000. Reward 62.000000. action: 0.000000. mean reward 76.782036.\n",
      "World Perf: Episode 517.000000. Reward 82.000000. action: 1.000000. mean reward 76.447395.\n",
      "World Perf: Episode 520.000000. Reward 66.000000. action: 1.000000. mean reward 78.720161.\n",
      "World Perf: Episode 523.000000. Reward 71.333333. action: 0.000000. mean reward 78.217674.\n",
      "World Perf: Episode 526.000000. Reward 72.666667. action: 1.000000. mean reward 77.986404.\n",
      "World Perf: Episode 529.000000. Reward 100.333333. action: 1.000000. mean reward 77.770683.\n",
      "World Perf: Episode 532.000000. Reward 90.666667. action: 0.000000. mean reward 78.600029.\n",
      "World Perf: Episode 535.000000. Reward 109.666667. action: 1.000000. mean reward 78.623634.\n",
      "World Perf: Episode 538.000000. Reward 77.333333. action: 0.000000. mean reward 80.756752.\n",
      "World Perf: Episode 541.000000. Reward 52.000000. action: 0.000000. mean reward 82.673790.\n",
      "World Perf: Episode 544.000000. Reward 101.666667. action: 1.000000. mean reward 82.289772.\n",
      "World Perf: Episode 547.000000. Reward 102.666667. action: 0.000000. mean reward 81.857506.\n",
      "World Perf: Episode 550.000000. Reward 109.666667. action: 1.000000. mean reward 81.508331.\n",
      "World Perf: Episode 553.000000. Reward 76.666667. action: 0.000000. mean reward 80.908417.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 556.000000. Reward 86.333333. action: 1.000000. mean reward 80.348656.\n",
      "World Perf: Episode 559.000000. Reward 62.333333. action: 1.000000. mean reward 81.857132.\n",
      "World Perf: Episode 562.000000. Reward 89.666667. action: 1.000000. mean reward 81.504982.\n",
      "World Perf: Episode 565.000000. Reward 143.666667. action: 1.000000. mean reward 83.817589.\n",
      "World Perf: Episode 568.000000. Reward 54.333333. action: 0.000000. mean reward 82.833023.\n",
      "World Perf: Episode 571.000000. Reward 120.000000. action: 1.000000. mean reward 82.778038.\n",
      "World Perf: Episode 574.000000. Reward 59.333333. action: 0.000000. mean reward 81.974899.\n",
      "World Perf: Episode 577.000000. Reward 52.333333. action: 1.000000. mean reward 83.904434.\n",
      "World Perf: Episode 580.000000. Reward 73.333333. action: 1.000000. mean reward 83.133347.\n",
      "World Perf: Episode 583.000000. Reward 125.000000. action: 0.000000. mean reward 82.839943.\n",
      "World Perf: Episode 586.000000. Reward 87.000000. action: 1.000000. mean reward 85.525917.\n",
      "World Perf: Episode 589.000000. Reward 145.000000. action: 0.000000. mean reward 88.344398.\n",
      "World Perf: Episode 592.000000. Reward 144.000000. action: 0.000000. mean reward 88.269257.\n",
      "World Perf: Episode 595.000000. Reward 129.666667. action: 1.000000. mean reward 88.065590.\n",
      "World Perf: Episode 598.000000. Reward 167.333333. action: 1.000000. mean reward 90.832649.\n",
      "World Perf: Episode 601.000000. Reward 110.000000. action: 1.000000. mean reward 90.188789.\n",
      "World Perf: Episode 604.000000. Reward 143.666667. action: 1.000000. mean reward 89.952332.\n",
      "World Perf: Episode 607.000000. Reward 159.333333. action: 1.000000. mean reward 90.766624.\n",
      "World Perf: Episode 610.000000. Reward 63.333333. action: 1.000000. mean reward 92.628731.\n",
      "World Perf: Episode 613.000000. Reward 113.666667. action: 1.000000. mean reward 92.243828.\n",
      "World Perf: Episode 616.000000. Reward 90.000000. action: 0.000000. mean reward 91.832207.\n",
      "World Perf: Episode 619.000000. Reward 94.333333. action: 1.000000. mean reward 94.024353.\n",
      "World Perf: Episode 622.000000. Reward 93.666667. action: 1.000000. mean reward 93.358986.\n",
      "World Perf: Episode 625.000000. Reward 186.000000. action: 0.000000. mean reward 93.776299.\n",
      "World Perf: Episode 628.000000. Reward 200.000000. action: 1.000000. mean reward 93.998466.\n",
      "World Perf: Episode 631.000000. Reward 82.666667. action: 1.000000. mean reward 95.800972.\n",
      "World Perf: Episode 634.000000. Reward 183.000000. action: 1.000000. mean reward 95.768585.\n",
      "World Perf: Episode 637.000000. Reward 127.333333. action: 0.000000. mean reward 95.303200.\n",
      "World Perf: Episode 640.000000. Reward 112.000000. action: 1.000000. mean reward 98.021843.\n",
      "World Perf: Episode 643.000000. Reward 123.000000. action: 0.000000. mean reward 99.583504.\n",
      "World Perf: Episode 646.000000. Reward 168.666667. action: 0.000000. mean reward 99.944244.\n",
      "World Perf: Episode 649.000000. Reward 130.333333. action: 1.000000. mean reward 102.292976.\n",
      "World Perf: Episode 652.000000. Reward 180.000000. action: 1.000000. mean reward 104.080933.\n",
      "World Perf: Episode 655.000000. Reward 164.666667. action: 0.000000. mean reward 105.635017.\n",
      "World Perf: Episode 658.000000. Reward 145.666667. action: 0.000000. mean reward 108.042351.\n",
      "World Perf: Episode 661.000000. Reward 134.000000. action: 0.000000. mean reward 107.453453.\n",
      "World Perf: Episode 664.000000. Reward 117.000000. action: 0.000000. mean reward 109.659706.\n",
      "World Perf: Episode 667.000000. Reward 200.000000. action: 0.000000. mean reward 110.014427.\n",
      "World Perf: Episode 670.000000. Reward 155.000000. action: 1.000000. mean reward 109.549538.\n",
      "World Perf: Episode 673.000000. Reward 125.666667. action: 0.000000. mean reward 109.737999.\n",
      "World Perf: Episode 676.000000. Reward 165.333333. action: 1.000000. mean reward 112.186958.\n",
      "World Perf: Episode 679.000000. Reward 164.666667. action: 1.000000. mean reward 112.755394.\n",
      "World Perf: Episode 682.000000. Reward 102.666667. action: 1.000000. mean reward 113.719841.\n",
      "World Perf: Episode 685.000000. Reward 156.000000. action: 0.000000. mean reward 113.901703.\n",
      "World Perf: Episode 688.000000. Reward 130.666667. action: 0.000000. mean reward 115.466194.\n",
      "World Perf: Episode 691.000000. Reward 116.666667. action: 1.000000. mean reward 117.366425.\n",
      "World Perf: Episode 694.000000. Reward 142.000000. action: 0.000000. mean reward 119.499786.\n",
      "World Perf: Episode 697.000000. Reward 200.000000. action: 0.000000. mean reward 119.778625.\n",
      "World Perf: Episode 700.000000. Reward 127.333333. action: 1.000000. mean reward 118.811317.\n",
      "World Perf: Episode 703.000000. Reward 168.333333. action: 1.000000. mean reward 118.320374.\n",
      "World Perf: Episode 706.000000. Reward 167.333333. action: 1.000000. mean reward 118.037323.\n",
      "World Perf: Episode 709.000000. Reward 168.000000. action: 1.000000. mean reward 117.797760.\n",
      "World Perf: Episode 712.000000. Reward 177.000000. action: 1.000000. mean reward 118.565071.\n",
      "World Perf: Episode 715.000000. Reward 190.333333. action: 1.000000. mean reward 120.357338.\n",
      "World Perf: Episode 718.000000. Reward 185.000000. action: 0.000000. mean reward 121.642975.\n",
      "World Perf: Episode 721.000000. Reward 184.333333. action: 0.000000. mean reward 122.754341.\n",
      "World Perf: Episode 724.000000. Reward 156.000000. action: 0.000000. mean reward 124.952583.\n",
      "World Perf: Episode 727.000000. Reward 147.333333. action: 0.000000. mean reward 124.286407.\n",
      "World Perf: Episode 730.000000. Reward 180.000000. action: 0.000000. mean reward 123.713127.\n",
      "World Perf: Episode 733.000000. Reward 200.000000. action: 0.000000. mean reward 126.256073.\n",
      "World Perf: Episode 736.000000. Reward 200.000000. action: 1.000000. mean reward 127.485588.\n",
      "World Perf: Episode 739.000000. Reward 200.000000. action: 1.000000. mean reward 129.321686.\n",
      "World Perf: Episode 742.000000. Reward 165.000000. action: 0.000000. mean reward 131.575790.\n",
      "World Perf: Episode 745.000000. Reward 122.333333. action: 0.000000. mean reward 132.160904.\n",
      "World Perf: Episode 748.000000. Reward 167.000000. action: 1.000000. mean reward 132.988190.\n",
      "World Perf: Episode 751.000000. Reward 200.000000. action: 0.000000. mean reward 134.372284.\n",
      "World Perf: Episode 754.000000. Reward 104.333333. action: 0.000000. mean reward 135.041992.\n",
      "World Perf: Episode 757.000000. Reward 151.333333. action: 1.000000. mean reward 134.806076.\n",
      "World Perf: Episode 760.000000. Reward 195.333333. action: 0.000000. mean reward 134.495529.\n",
      "World Perf: Episode 763.000000. Reward 143.000000. action: 1.000000. mean reward 133.468307.\n",
      "World Perf: Episode 766.000000. Reward 155.000000. action: 1.000000. mean reward 132.972427.\n",
      "World Perf: Episode 769.000000. Reward 127.666667. action: 0.000000. mean reward 131.983917.\n",
      "World Perf: Episode 772.000000. Reward 192.666667. action: 0.000000. mean reward 133.888992.\n",
      "World Perf: Episode 775.000000. Reward 150.000000. action: 1.000000. mean reward 134.692657.\n",
      "World Perf: Episode 778.000000. Reward 178.333333. action: 1.000000. mean reward 133.967575.\n",
      "World Perf: Episode 781.000000. Reward 200.000000. action: 1.000000. mean reward 133.618332.\n",
      "World Perf: Episode 784.000000. Reward 162.333333. action: 1.000000. mean reward 132.718704.\n",
      "World Perf: Episode 787.000000. Reward 187.333333. action: 1.000000. mean reward 132.346909.\n",
      "World Perf: Episode 790.000000. Reward 190.000000. action: 1.000000. mean reward 132.061172.\n",
      "World Perf: Episode 793.000000. Reward 118.666667. action: 1.000000. mean reward 133.540955.\n",
      "World Perf: Episode 796.000000. Reward 148.666667. action: 0.000000. mean reward 134.766098.\n",
      "World Perf: Episode 799.000000. Reward 172.333333. action: 1.000000. mean reward 134.404800.\n",
      "World Perf: Episode 802.000000. Reward 127.000000. action: 1.000000. mean reward 135.146072.\n",
      "World Perf: Episode 805.000000. Reward 184.333333. action: 1.000000. mean reward 134.731827.\n",
      "World Perf: Episode 808.000000. Reward 192.666667. action: 1.000000. mean reward 134.303726.\n",
      "World Perf: Episode 811.000000. Reward 200.000000. action: 0.000000. mean reward 136.686996.\n",
      "World Perf: Episode 814.000000. Reward 200.000000. action: 0.000000. mean reward 137.623596.\n",
      "World Perf: Episode 817.000000. Reward 136.333333. action: 1.000000. mean reward 136.721252.\n",
      "World Perf: Episode 820.000000. Reward 138.333333. action: 1.000000. mean reward 136.250092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 823.000000. Reward 154.000000. action: 1.000000. mean reward 137.041428.\n",
      "World Perf: Episode 826.000000. Reward 178.666667. action: 1.000000. mean reward 136.649124.\n",
      "World Perf: Episode 829.000000. Reward 188.333333. action: 0.000000. mean reward 136.369354.\n",
      "World Perf: Episode 832.000000. Reward 167.333333. action: 0.000000. mean reward 136.077423.\n",
      "World Perf: Episode 835.000000. Reward 177.666667. action: 0.000000. mean reward 136.086380.\n",
      "World Perf: Episode 838.000000. Reward 181.000000. action: 0.000000. mean reward 135.544907.\n",
      "World Perf: Episode 841.000000. Reward 140.000000. action: 1.000000. mean reward 134.994034.\n",
      "World Perf: Episode 844.000000. Reward 200.000000. action: 1.000000. mean reward 135.044754.\n",
      "World Perf: Episode 847.000000. Reward 200.000000. action: 1.000000. mean reward 134.976181.\n",
      "World Perf: Episode 850.000000. Reward 177.333333. action: 1.000000. mean reward 136.170700.\n",
      "World Perf: Episode 853.000000. Reward 165.333333. action: 0.000000. mean reward 135.825943.\n",
      "World Perf: Episode 856.000000. Reward 200.000000. action: 1.000000. mean reward 136.311295.\n",
      "World Perf: Episode 859.000000. Reward 176.666667. action: 1.000000. mean reward 136.208237.\n",
      "World Perf: Episode 862.000000. Reward 179.000000. action: 1.000000. mean reward 136.926529.\n",
      "World Perf: Episode 865.000000. Reward 178.666667. action: 0.000000. mean reward 136.680222.\n",
      "World Perf: Episode 868.000000. Reward 131.666667. action: 1.000000. mean reward 135.628067.\n",
      "World Perf: Episode 871.000000. Reward 169.000000. action: 0.000000. mean reward 135.279831.\n",
      "World Perf: Episode 874.000000. Reward 200.000000. action: 1.000000. mean reward 135.289597.\n",
      "World Perf: Episode 877.000000. Reward 170.666667. action: 1.000000. mean reward 136.455872.\n",
      "World Perf: Episode 880.000000. Reward 130.333333. action: 0.000000. mean reward 135.452286.\n",
      "World Perf: Episode 883.000000. Reward 185.000000. action: 1.000000. mean reward 136.680176.\n",
      "World Perf: Episode 886.000000. Reward 196.666667. action: 1.000000. mean reward 136.278366.\n",
      "World Perf: Episode 889.000000. Reward 135.000000. action: 1.000000. mean reward 135.123611.\n",
      "World Perf: Episode 892.000000. Reward 184.666667. action: 0.000000. mean reward 137.209198.\n",
      "World Perf: Episode 895.000000. Reward 178.333333. action: 0.000000. mean reward 138.310196.\n",
      "World Perf: Episode 898.000000. Reward 176.333333. action: 1.000000. mean reward 137.874756.\n",
      "World Perf: Episode 901.000000. Reward 129.666667. action: 1.000000. mean reward 136.745377.\n",
      "World Perf: Episode 904.000000. Reward 200.000000. action: 0.000000. mean reward 136.223175.\n",
      "World Perf: Episode 907.000000. Reward 200.000000. action: 0.000000. mean reward 136.919937.\n",
      "World Perf: Episode 910.000000. Reward 157.666667. action: 1.000000. mean reward 137.961182.\n",
      "World Perf: Episode 913.000000. Reward 200.000000. action: 0.000000. mean reward 137.444565.\n",
      "World Perf: Episode 916.000000. Reward 172.000000. action: 1.000000. mean reward 137.680771.\n",
      "World Perf: Episode 919.000000. Reward 153.666667. action: 0.000000. mean reward 138.455322.\n",
      "World Perf: Episode 922.000000. Reward 143.666667. action: 1.000000. mean reward 138.103561.\n",
      "World Perf: Episode 925.000000. Reward 156.333333. action: 0.000000. mean reward 139.846359.\n",
      "World Perf: Episode 928.000000. Reward 200.000000. action: 1.000000. mean reward 141.992859.\n",
      "World Perf: Episode 931.000000. Reward 200.000000. action: 1.000000. mean reward 144.159103.\n",
      "World Perf: Episode 934.000000. Reward 133.333333. action: 1.000000. mean reward 143.882324.\n",
      "World Perf: Episode 937.000000. Reward 200.000000. action: 0.000000. mean reward 145.078232.\n",
      "World Perf: Episode 940.000000. Reward 175.000000. action: 1.000000. mean reward 144.807465.\n",
      "World Perf: Episode 943.000000. Reward 169.666667. action: 0.000000. mean reward 146.126373.\n",
      "World Perf: Episode 946.000000. Reward 175.000000. action: 1.000000. mean reward 145.461868.\n",
      "World Perf: Episode 949.000000. Reward 180.000000. action: 1.000000. mean reward 144.937820.\n",
      "World Perf: Episode 952.000000. Reward 168.333333. action: 0.000000. mean reward 146.743195.\n",
      "World Perf: Episode 955.000000. Reward 200.000000. action: 0.000000. mean reward 148.764725.\n",
      "World Perf: Episode 958.000000. Reward 197.666667. action: 1.000000. mean reward 150.076645.\n",
      "World Perf: Episode 961.000000. Reward 200.000000. action: 0.000000. mean reward 151.616913.\n",
      "World Perf: Episode 964.000000. Reward 200.000000. action: 0.000000. mean reward 153.577911.\n",
      "World Perf: Episode 967.000000. Reward 168.000000. action: 0.000000. mean reward 155.152817.\n",
      "World Perf: Episode 970.000000. Reward 188.000000. action: 1.000000. mean reward 156.974716.\n",
      "World Perf: Episode 973.000000. Reward 200.000000. action: 0.000000. mean reward 156.813263.\n",
      "World Perf: Episode 976.000000. Reward 200.000000. action: 0.000000. mean reward 157.149338.\n",
      "World Perf: Episode 979.000000. Reward 200.000000. action: 1.000000. mean reward 159.039932.\n",
      "World Perf: Episode 982.000000. Reward 193.000000. action: 1.000000. mean reward 160.847183.\n",
      "World Perf: Episode 985.000000. Reward 200.000000. action: 0.000000. mean reward 160.320663.\n",
      "World Perf: Episode 988.000000. Reward 200.000000. action: 0.000000. mean reward 160.929901.\n",
      "World Perf: Episode 991.000000. Reward 195.000000. action: 1.000000. mean reward 160.476059.\n",
      "World Perf: Episode 994.000000. Reward 200.000000. action: 1.000000. mean reward 162.288254.\n",
      "World Perf: Episode 997.000000. Reward 140.333333. action: 1.000000. mean reward 161.538086.\n",
      "World Perf: Episode 1000.000000. Reward 200.000000. action: 1.000000. mean reward 161.428818.\n",
      "World Perf: Episode 1003.000000. Reward 153.000000. action: 0.000000. mean reward 160.305420.\n",
      "World Perf: Episode 1006.000000. Reward 200.000000. action: 1.000000. mean reward 160.018738.\n",
      "World Perf: Episode 1009.000000. Reward 200.000000. action: 1.000000. mean reward 159.355179.\n",
      "World Perf: Episode 1012.000000. Reward 200.000000. action: 0.000000. mean reward 158.705856.\n",
      "World Perf: Episode 1015.000000. Reward 166.666667. action: 0.000000. mean reward 160.198990.\n",
      "World Perf: Episode 1018.000000. Reward 200.000000. action: 1.000000. mean reward 161.326660.\n",
      "World Perf: Episode 1021.000000. Reward 185.666667. action: 0.000000. mean reward 160.277054.\n",
      "World Perf: Episode 1024.000000. Reward 200.000000. action: 0.000000. mean reward 160.830490.\n",
      "World Perf: Episode 1027.000000. Reward 200.000000. action: 0.000000. mean reward 160.317429.\n",
      "World Perf: Episode 1030.000000. Reward 200.000000. action: 0.000000. mean reward 160.565231.\n",
      "World Perf: Episode 1033.000000. Reward 200.000000. action: 1.000000. mean reward 159.908966.\n",
      "World Perf: Episode 1036.000000. Reward 200.000000. action: 1.000000. mean reward 161.199203.\n",
      "World Perf: Episode 1039.000000. Reward 134.666667. action: 0.000000. mean reward 161.353882.\n",
      "World Perf: Episode 1042.000000. Reward 142.666667. action: 0.000000. mean reward 159.821671.\n",
      "World Perf: Episode 1045.000000. Reward 180.333333. action: 1.000000. mean reward 159.422775.\n",
      "World Perf: Episode 1048.000000. Reward 200.000000. action: 1.000000. mean reward 160.134995.\n",
      "World Perf: Episode 1051.000000. Reward 200.000000. action: 0.000000. mean reward 159.795929.\n",
      "World Perf: Episode 1054.000000. Reward 200.000000. action: 0.000000. mean reward 159.464859.\n",
      "World Perf: Episode 1057.000000. Reward 192.666667. action: 0.000000. mean reward 158.612823.\n",
      "World Perf: Episode 1060.000000. Reward 99.666667. action: 0.000000. mean reward 157.390610.\n",
      "World Perf: Episode 1063.000000. Reward 193.666667. action: 0.000000. mean reward 158.472458.\n",
      "World Perf: Episode 1066.000000. Reward 200.000000. action: 1.000000. mean reward 157.867691.\n",
      "World Perf: Episode 1069.000000. Reward 200.000000. action: 1.000000. mean reward 157.365585.\n",
      "World Perf: Episode 1072.000000. Reward 200.000000. action: 1.000000. mean reward 159.243484.\n",
      "World Perf: Episode 1075.000000. Reward 200.000000. action: 0.000000. mean reward 158.718307.\n",
      "World Perf: Episode 1078.000000. Reward 200.000000. action: 1.000000. mean reward 159.653473.\n",
      "World Perf: Episode 1081.000000. Reward 200.000000. action: 1.000000. mean reward 161.471603.\n",
      "World Perf: Episode 1084.000000. Reward 200.000000. action: 1.000000. mean reward 162.432129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1087.000000. Reward 200.000000. action: 1.000000. mean reward 161.652756.\n",
      "World Perf: Episode 1090.000000. Reward 200.000000. action: 1.000000. mean reward 161.443008.\n",
      "World Perf: Episode 1093.000000. Reward 200.000000. action: 1.000000. mean reward 163.164688.\n",
      "World Perf: Episode 1096.000000. Reward 200.000000. action: 0.000000. mean reward 164.540756.\n",
      "World Perf: Episode 1099.000000. Reward 200.000000. action: 1.000000. mean reward 166.056992.\n",
      "World Perf: Episode 1102.000000. Reward 200.000000. action: 1.000000. mean reward 167.523438.\n",
      "World Perf: Episode 1105.000000. Reward 200.000000. action: 1.000000. mean reward 169.166565.\n",
      "World Perf: Episode 1108.000000. Reward 200.000000. action: 1.000000. mean reward 170.043915.\n",
      "World Perf: Episode 1111.000000. Reward 189.000000. action: 0.000000. mean reward 171.524368.\n",
      "World Perf: Episode 1114.000000. Reward 200.000000. action: 0.000000. mean reward 172.356491.\n",
      "World Perf: Episode 1117.000000. Reward 200.000000. action: 1.000000. mean reward 173.913528.\n",
      "World Perf: Episode 1120.000000. Reward 191.333333. action: 1.000000. mean reward 175.323441.\n",
      "World Perf: Episode 1123.000000. Reward 200.000000. action: 1.000000. mean reward 176.802658.\n",
      "World Perf: Episode 1126.000000. Reward 200.000000. action: 1.000000. mean reward 177.609787.\n",
      "World Perf: Episode 1129.000000. Reward 200.000000. action: 0.000000. mean reward 178.306656.\n",
      "World Perf: Episode 1132.000000. Reward 200.000000. action: 1.000000. mean reward 179.744095.\n",
      "World Perf: Episode 1135.000000. Reward 195.666667. action: 0.000000. mean reward 179.181625.\n",
      "World Perf: Episode 1138.000000. Reward 200.000000. action: 0.000000. mean reward 178.211914.\n",
      "World Perf: Episode 1141.000000. Reward 200.000000. action: 0.000000. mean reward 177.196106.\n",
      "World Perf: Episode 1144.000000. Reward 200.000000. action: 0.000000. mean reward 176.228134.\n",
      "World Perf: Episode 1147.000000. Reward 200.000000. action: 1.000000. mean reward 175.449875.\n",
      "World Perf: Episode 1150.000000. Reward 200.000000. action: 0.000000. mean reward 176.012985.\n",
      "World Perf: Episode 1153.000000. Reward 200.000000. action: 0.000000. mean reward 175.909729.\n",
      "World Perf: Episode 1156.000000. Reward 200.000000. action: 1.000000. mean reward 177.240356.\n",
      "World Perf: Episode 1159.000000. Reward 200.000000. action: 0.000000. mean reward 178.632004.\n",
      "World Perf: Episode 1162.000000. Reward 200.000000. action: 1.000000. mean reward 177.768326.\n",
      "World Perf: Episode 1165.000000. Reward 200.000000. action: 1.000000. mean reward 176.407715.\n",
      "World Perf: Episode 1168.000000. Reward 200.000000. action: 0.000000. mean reward 176.762817.\n",
      "World Perf: Episode 1171.000000. Reward 200.000000. action: 1.000000. mean reward 176.452942.\n",
      "World Perf: Episode 1174.000000. Reward 200.000000. action: 1.000000. mean reward 177.384048.\n",
      "World Perf: Episode 1177.000000. Reward 200.000000. action: 0.000000. mean reward 177.179642.\n",
      "World Perf: Episode 1180.000000. Reward 200.000000. action: 0.000000. mean reward 175.825485.\n",
      "World Perf: Episode 1183.000000. Reward 200.000000. action: 1.000000. mean reward 177.099747.\n",
      "World Perf: Episode 1186.000000. Reward 200.000000. action: 1.000000. mean reward 175.675247.\n",
      "World Perf: Episode 1189.000000. Reward 200.000000. action: 1.000000. mean reward 174.778885.\n",
      "World Perf: Episode 1192.000000. Reward 200.000000. action: 1.000000. mean reward 175.336411.\n",
      "World Perf: Episode 1195.000000. Reward 200.000000. action: 0.000000. mean reward 174.359375.\n",
      "World Perf: Episode 1198.000000. Reward 200.000000. action: 0.000000. mean reward 173.964249.\n",
      "World Perf: Episode 1201.000000. Reward 200.000000. action: 1.000000. mean reward 172.983948.\n",
      "World Perf: Episode 1204.000000. Reward 200.000000. action: 0.000000. mean reward 173.190247.\n",
      "World Perf: Episode 1207.000000. Reward 200.000000. action: 1.000000. mean reward 172.072876.\n",
      "World Perf: Episode 1210.000000. Reward 200.000000. action: 1.000000. mean reward 171.388062.\n",
      "World Perf: Episode 1213.000000. Reward 172.666667. action: 0.000000. mean reward 172.580307.\n",
      "World Perf: Episode 1216.000000. Reward 200.000000. action: 0.000000. mean reward 171.516769.\n",
      "World Perf: Episode 1219.000000. Reward 200.000000. action: 0.000000. mean reward 173.173721.\n",
      "World Perf: Episode 1222.000000. Reward 200.000000. action: 0.000000. mean reward 174.859970.\n",
      "World Perf: Episode 1225.000000. Reward 200.000000. action: 0.000000. mean reward 175.389389.\n",
      "World Perf: Episode 1228.000000. Reward 200.000000. action: 1.000000. mean reward 176.884094.\n",
      "World Perf: Episode 1231.000000. Reward 200.000000. action: 0.000000. mean reward 178.351822.\n",
      "World Perf: Episode 1234.000000. Reward 200.000000. action: 1.000000. mean reward 177.944992.\n",
      "World Perf: Episode 1237.000000. Reward 200.000000. action: 1.000000. mean reward 178.729477.\n",
      "World Perf: Episode 1240.000000. Reward 200.000000. action: 1.000000. mean reward 178.872452.\n",
      "World Perf: Episode 1243.000000. Reward 200.000000. action: 0.000000. mean reward 178.088852.\n",
      "World Perf: Episode 1246.000000. Reward 200.000000. action: 0.000000. mean reward 176.848755.\n",
      "World Perf: Episode 1249.000000. Reward 200.000000. action: 1.000000. mean reward 176.759827.\n",
      "World Perf: Episode 1252.000000. Reward 200.000000. action: 0.000000. mean reward 176.260010.\n",
      "World Perf: Episode 1255.000000. Reward 200.000000. action: 1.000000. mean reward 177.678940.\n",
      "World Perf: Episode 1258.000000. Reward 200.000000. action: 0.000000. mean reward 178.205139.\n",
      "World Perf: Episode 1261.000000. Reward 200.000000. action: 0.000000. mean reward 177.059036.\n",
      "World Perf: Episode 1264.000000. Reward 184.333333. action: 0.000000. mean reward 177.453262.\n",
      "World Perf: Episode 1267.000000. Reward 150.333333. action: 0.000000. mean reward 177.458618.\n",
      "World Perf: Episode 1270.000000. Reward 149.000000. action: 1.000000. mean reward 175.677719.\n",
      "World Perf: Episode 1273.000000. Reward 200.000000. action: 1.000000. mean reward 174.477768.\n",
      "World Perf: Episode 1276.000000. Reward 174.333333. action: 0.000000. mean reward 173.026718.\n",
      "World Perf: Episode 1279.000000. Reward 200.000000. action: 1.000000. mean reward 172.772034.\n",
      "World Perf: Episode 1282.000000. Reward 200.000000. action: 1.000000. mean reward 171.643295.\n",
      "World Perf: Episode 1285.000000. Reward 200.000000. action: 0.000000. mean reward 170.785080.\n",
      "World Perf: Episode 1288.000000. Reward 200.000000. action: 1.000000. mean reward 170.710938.\n",
      "World Perf: Episode 1291.000000. Reward 200.000000. action: 1.000000. mean reward 169.730972.\n",
      "World Perf: Episode 1294.000000. Reward 140.000000. action: 0.000000. mean reward 170.117676.\n",
      "World Perf: Episode 1297.000000. Reward 200.000000. action: 1.000000. mean reward 170.256577.\n",
      "World Perf: Episode 1300.000000. Reward 191.333333. action: 0.000000. mean reward 170.945312.\n",
      "World Perf: Episode 1303.000000. Reward 200.000000. action: 0.000000. mean reward 172.574417.\n",
      "World Perf: Episode 1306.000000. Reward 200.000000. action: 0.000000. mean reward 172.330978.\n",
      "World Perf: Episode 1309.000000. Reward 200.000000. action: 0.000000. mean reward 173.920151.\n",
      "World Perf: Episode 1312.000000. Reward 200.000000. action: 1.000000. mean reward 175.437881.\n",
      "World Perf: Episode 1315.000000. Reward 200.000000. action: 0.000000. mean reward 176.916824.\n",
      "World Perf: Episode 1318.000000. Reward 172.666667. action: 1.000000. mean reward 178.091171.\n",
      "World Perf: Episode 1321.000000. Reward 200.000000. action: 1.000000. mean reward 178.571030.\n",
      "World Perf: Episode 1324.000000. Reward 200.000000. action: 1.000000. mean reward 180.270142.\n",
      "World Perf: Episode 1327.000000. Reward 200.000000. action: 0.000000. mean reward 181.315170.\n",
      "World Perf: Episode 1330.000000. Reward 200.000000. action: 1.000000. mean reward 179.992752.\n",
      "World Perf: Episode 1333.000000. Reward 200.000000. action: 1.000000. mean reward 178.593430.\n",
      "World Perf: Episode 1336.000000. Reward 200.000000. action: 1.000000. mean reward 179.715317.\n",
      "World Perf: Episode 1339.000000. Reward 200.000000. action: 1.000000. mean reward 178.280136.\n",
      "World Perf: Episode 1342.000000. Reward 200.000000. action: 0.000000. mean reward 177.594620.\n",
      "World Perf: Episode 1345.000000. Reward 200.000000. action: 0.000000. mean reward 178.018127.\n",
      "World Perf: Episode 1348.000000. Reward 200.000000. action: 0.000000. mean reward 179.486496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1351.000000. Reward 200.000000. action: 0.000000. mean reward 179.964340.\n",
      "World Perf: Episode 1354.000000. Reward 176.000000. action: 0.000000. mean reward 178.336075.\n",
      "World Perf: Episode 1357.000000. Reward 200.000000. action: 1.000000. mean reward 178.248779.\n",
      "World Perf: Episode 1360.000000. Reward 200.000000. action: 1.000000. mean reward 178.696289.\n",
      "World Perf: Episode 1363.000000. Reward 200.000000. action: 0.000000. mean reward 177.284592.\n",
      "World Perf: Episode 1366.000000. Reward 200.000000. action: 0.000000. mean reward 176.093872.\n",
      "World Perf: Episode 1369.000000. Reward 200.000000. action: 0.000000. mean reward 178.014511.\n",
      "World Perf: Episode 1372.000000. Reward 200.000000. action: 0.000000. mean reward 176.608261.\n",
      "World Perf: Episode 1375.000000. Reward 200.000000. action: 1.000000. mean reward 177.300964.\n",
      "World Perf: Episode 1378.000000. Reward 200.000000. action: 0.000000. mean reward 178.744812.\n",
      "World Perf: Episode 1381.000000. Reward 200.000000. action: 1.000000. mean reward 179.973129.\n",
      "World Perf: Episode 1384.000000. Reward 200.000000. action: 0.000000. mean reward 181.339722.\n",
      "World Perf: Episode 1387.000000. Reward 200.000000. action: 1.000000. mean reward 182.601303.\n",
      "World Perf: Episode 1390.000000. Reward 200.000000. action: 1.000000. mean reward 182.530640.\n",
      "World Perf: Episode 1393.000000. Reward 200.000000. action: 1.000000. mean reward 181.116531.\n",
      "World Perf: Episode 1396.000000. Reward 200.000000. action: 1.000000. mean reward 179.632202.\n",
      "World Perf: Episode 1399.000000. Reward 200.000000. action: 1.000000. mean reward 178.504349.\n",
      "World Perf: Episode 1402.000000. Reward 200.000000. action: 0.000000. mean reward 177.335098.\n",
      "World Perf: Episode 1405.000000. Reward 200.000000. action: 0.000000. mean reward 178.481018.\n",
      "World Perf: Episode 1408.000000. Reward 200.000000. action: 0.000000. mean reward 179.838623.\n",
      "World Perf: Episode 1411.000000. Reward 200.000000. action: 1.000000. mean reward 180.997635.\n",
      "World Perf: Episode 1414.000000. Reward 200.000000. action: 1.000000. mean reward 182.387466.\n",
      "World Perf: Episode 1417.000000. Reward 200.000000. action: 1.000000. mean reward 183.723572.\n",
      "World Perf: Episode 1420.000000. Reward 200.000000. action: 0.000000. mean reward 182.768188.\n",
      "World Perf: Episode 1423.000000. Reward 200.000000. action: 0.000000. mean reward 181.885361.\n",
      "World Perf: Episode 1426.000000. Reward 200.000000. action: 1.000000. mean reward 181.108932.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-930a7e44d2a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-97bcbf2fd434>\u001b[0m in \u001b[0;36mstepModel\u001b[0;34m(sess, xs, action)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtoFeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmyPredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoFeed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyPredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyPredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.initialize_all_variables()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "#                 if episode_number > 100:\n",
    "#                     drawFromModel = not drawFromModel\n",
    "#                     trainTheModel = not trainTheModel\n",
    "#                     trainThePolicy = not trainThePolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
